{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Pandas Data-Structures\n",
    "\n",
    "# Single-dimentional data structure       \n",
    "#     < Series Object > \n",
    "\n",
    "s1 = pd.Series([10,20,30,40,50])\n",
    "# print(s1)\n",
    "    # 0    10\n",
    "    # 1    20\n",
    "    # 2    30\n",
    "    # 3    40\n",
    "    # 4    50\n",
    "s1 = pd.Series([10,20,30,40,50], index=['a','b','c','d','e'])\n",
    "# print(s1)  \n",
    "    # a    10\n",
    "    # b    20\n",
    "    # c    30\n",
    "    # d    40\n",
    "    # e    50\n",
    "\n",
    "sd1 = pd.Series({'a':10,'b':20,'c':30})  # if no index given, key will be the default index\n",
    "# print(sd1)\n",
    "    # a    10\n",
    "    # b    20\n",
    "    # c    30\n",
    "\n",
    "    \n",
    "    \n",
    "#  Multi-dimentional data structure\n",
    "\n",
    "#                                    head()\n",
    "#\n",
    "#  shape()        < DataFrame Object >         describe()\n",
    "#\n",
    "#                                    tail()\n",
    "\n",
    "\n",
    "student = {\"student_name\":['Bob', 'Sam', 'Julia', 'Charles'], \"Sstudent_marks\":[83,13,99,47]}\n",
    "                     # 'key' becomes column name,  'values' become row values\n",
    "df1 = pd.DataFrame(student)\n",
    "    #     student_name    Sstudent_marks\n",
    "    # 0       Bob                       83\n",
    "    # 1       Sam                       13\n",
    "    # 2       Julia                     99\n",
    "    # 3       Charles                47\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>Name</th>\n",
       "      <th>Rep</th>\n",
       "      <th>Manager</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>688981</td>\n",
       "      <td>Keeling LLC</td>\n",
       "      <td>Wendy Yule</td>\n",
       "      <td>Fred Anderson</td>\n",
       "      <td>CPU</td>\n",
       "      <td>5</td>\n",
       "      <td>100000</td>\n",
       "      <td>won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>729833</td>\n",
       "      <td>Koepp Ltd</td>\n",
       "      <td>Wendy Yule</td>\n",
       "      <td>Fred Anderson</td>\n",
       "      <td>CPU</td>\n",
       "      <td>2</td>\n",
       "      <td>65000</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Account         Name         Rep        Manager Product  Quantity   Price  \\\n",
       "14   688981  Keeling LLC  Wendy Yule  Fred Anderson     CPU         5  100000   \n",
       "15   729833    Koepp Ltd  Wendy Yule  Fred Anderson     CPU         2   65000   \n",
       "\n",
       "      Status  \n",
       "14       won  \n",
       "15  declined  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"data/sales-funnel.csv\")\n",
    "# df.head()\n",
    "# df.columns\n",
    "# df.index\n",
    "# df.values\n",
    "# type(df)\n",
    "# df.shape  # df.shape[0]     df.shape[1]\n",
    "# df.info()\n",
    "\n",
    "df.iloc[0:3,0:2]   # index location [rows, columns]\n",
    "df1 = df.iloc[9:13,2:4]\n",
    "df1\n",
    "\n",
    "df.loc[0:3, ('Rep', 'Manager')]\n",
    "\n",
    "df['Price'] > 40000\n",
    "df[ df['Price'] > 40000 ]\n",
    "df[ (df['Price'] > 40000) & (df['Rep'] == 'Wendy Yule') ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "carrier      0\n",
      "tailnum    248\n",
      "origin       0\n",
      "dest         0\n",
      "dtype: int64\n",
      "AS    62460\n",
      "WN    23355\n",
      "OO    18710\n",
      "DL    16716\n",
      "UA    16671\n",
      "AA     7586\n",
      "US     5946\n",
      "B6     3540\n",
      "VX     3272\n",
      "F9     2698\n",
      "HA     1095\n",
      "Name: carrier, dtype: int64\n",
      "11\n",
      "   carrier tailnum origin dest\n",
      "0        2  N508AS    PDX  ANC\n",
      "1        9  N195UW    SEA  CLT\n",
      "2        8  N37422    PDX  IAH\n",
      "3        9  N547UW    PDX  CLT\n",
      "4        2  N762AS    SEA  ANC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carrier</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>US_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS</td>\n",
       "      <td>N508AS</td>\n",
       "      <td>PDX</td>\n",
       "      <td>ANC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>N195UW</td>\n",
       "      <td>SEA</td>\n",
       "      <td>CLT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA</td>\n",
       "      <td>N37422</td>\n",
       "      <td>PDX</td>\n",
       "      <td>IAH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>N547UW</td>\n",
       "      <td>PDX</td>\n",
       "      <td>CLT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AS</td>\n",
       "      <td>N762AS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>ANC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  carrier tailnum origin dest  US_code\n",
       "0      AS  N508AS    PDX  ANC        0\n",
       "1      US  N195UW    SEA  CLT        1\n",
       "2      UA  N37422    PDX  IAH        0\n",
       "3      US  N547UW    PDX  CLT        1\n",
       "4      AS  N762AS    SEA  ANC        0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# !pip install jupyter_contrib_nbextensions && jupyter contrib nbextension install\n",
    "# !jupyter notebook --version\n",
    "# !pip install --upgrade ipython\n",
    "# !jupyter --path\n",
    "# !pip freeze\n",
    "\n",
    "df_flights = pd.read_csv('https://raw.githubusercontent.com/ismayc/pnwflights14/master/data/flights.csv')\n",
    "# df_flights.to_csv('datax/flights.txt')\n",
    "# df_flights.head()\n",
    "# print(df_flights.info())\n",
    "\n",
    "# df_flights.boxplot('dep_time','origin',rot = 30,figsize=(5,6))\n",
    "\n",
    "cat_df_flights = df_flights.select_dtypes(include=['object']).copy()\n",
    "cat_df_flights.head()\n",
    "\n",
    "#total number of missing values in the DataFrame\n",
    "print(cat_df_flights.isnull().values.sum())\n",
    "#column-wise distribution of null values\n",
    "print(cat_df_flights.isnull().sum())\n",
    "\n",
    "print(cat_df_flights['carrier'].value_counts())\n",
    "print(cat_df_flights['carrier'].value_counts().count())\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # bar chart\n",
    "# carrier_count = cat_df_flights['carrier'].value_counts()\n",
    "# sns.set(style=\"darkgrid\")\n",
    "# sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n",
    "# plt.title('Frequency Distribution of Carriers')\n",
    "# plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "# plt.xlabel('Carrier', fontsize=12)\n",
    "# plt.show()\n",
    "\n",
    "# # pie chart\n",
    "# labels = cat_df_flights['carrier'].astype('category').cat.categories.tolist()\n",
    "# counts = cat_df_flights['carrier'].value_counts()\n",
    "# sizes = [counts[var_cat] for var_cat in labels]\n",
    "# fig1, ax1 = plt.subplots()\n",
    "# ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\n",
    "# ax1.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# < Encoding Categorical Data >\n",
    "\n",
    "#-Replacing values                 ( By default the categories are ordered alphabetically )\n",
    "                                                    # typecasting by using .astype() method on your columns\n",
    "labels = cat_df_flights['carrier'].astype('category').cat.categories.tolist()\n",
    "replace_map_comp = {'carrier' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
    "# print(replace_map_comp)\n",
    "cat_df_flights_repl = cat_df_flights.copy()\n",
    "cat_df_flights_repl.replace(replace_map_comp, inplace=True)\n",
    "print(cat_df_flights_repl.head())\n",
    "\n",
    "\n",
    "cat_df_flights_lc = cat_df_flights.copy()\n",
    "cat_df_flights_lc['carrier'] = cat_df_flights_lc['carrier'].astype('category')\n",
    "cat_df_flights_lc['origin'] = cat_df_flights_lc['origin'].astype('category')\n",
    "\n",
    "# import time\n",
    "# %timeit cat_df_flights.groupby(['origin','carrier']).count() #DataFrame with object dtype columns\n",
    "\n",
    "# %timeit cat_df_flights_lc.groupby(['origin','carrier']).count() #DataFrame with category dtype columns\n",
    "\n",
    "#-Encoding labels\n",
    "cat_df_flights_lc['carrier'] = cat_df_flights_lc['carrier'].cat.codes\n",
    "cat_df_flights_lc.head() #alphabetically labeled from 0 to 10\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cat_df_flights_specific = cat_df_flights.copy()\n",
    "cat_df_flights_specific['US_code'] = np.where(cat_df_flights_specific['carrier'].str.contains('US'), 1, 0)\n",
    "\n",
    "cat_df_flights_specific.head()\n",
    "\n",
    "\n",
    "\n",
    "#-One-Hot encoding\n",
    "\n",
    "\n",
    "#-Binary encoding\n",
    "\n",
    "#-Backward difference encoding\n",
    "\n",
    "#-Miscellaneous features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>element</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>d30</td>\n",
       "      <td>27.8</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>d11</td>\n",
       "      <td>29.7</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>d2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>d23</td>\n",
       "      <td>29.9</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>d3</td>\n",
       "      <td>24.1</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>d10</td>\n",
       "      <td>34.5</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>d16</td>\n",
       "      <td>31.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>d5</td>\n",
       "      <td>32.1</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>d27</td>\n",
       "      <td>36.3</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>d27</td>\n",
       "      <td>33.2</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>d17</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>d29</td>\n",
       "      <td>30.1</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>d3</td>\n",
       "      <td>28.6</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>d14</td>\n",
       "      <td>29.9</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d23</td>\n",
       "      <td>26.4</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d5</td>\n",
       "      <td>29.6</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d29</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d13</td>\n",
       "      <td>29.8</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d25</td>\n",
       "      <td>29.7</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d31</td>\n",
       "      <td>25.4</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>8</td>\n",
       "      <td>d8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>d5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>d14</td>\n",
       "      <td>29.5</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>d15</td>\n",
       "      <td>28.7</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>d28</td>\n",
       "      <td>31.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>d7</td>\n",
       "      <td>28.1</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>d2</td>\n",
       "      <td>31.3</td>\n",
       "      <td>16.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>d5</td>\n",
       "      <td>26.3</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>d27</td>\n",
       "      <td>27.7</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>d26</td>\n",
       "      <td>28.1</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>d4</td>\n",
       "      <td>27.2</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>d1</td>\n",
       "      <td>29.9</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MX17004</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>d6</td>\n",
       "      <td>27.8</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "element       id  year  month  day  tmax  tmin\n",
       "0        MX17004  2010      1  d30  27.8  14.5\n",
       "1        MX17004  2010      2  d11  29.7  13.4\n",
       "2        MX17004  2010      2   d2  27.3  14.4\n",
       "3        MX17004  2010      2  d23  29.9  10.7\n",
       "4        MX17004  2010      2   d3  24.1  14.4\n",
       "5        MX17004  2010      3  d10  34.5  16.8\n",
       "6        MX17004  2010      3  d16  31.1  17.6\n",
       "7        MX17004  2010      3   d5  32.1  14.2\n",
       "8        MX17004  2010      4  d27  36.3  16.7\n",
       "9        MX17004  2010      5  d27  33.2  18.2\n",
       "10       MX17004  2010      6  d17  28.0  17.5\n",
       "11       MX17004  2010      6  d29  30.1  18.0\n",
       "12       MX17004  2010      7   d3  28.6  17.5\n",
       "13       MX17004  2010      7  d14  29.9  16.5\n",
       "14       MX17004  2010      8  d23  26.4  15.0\n",
       "15       MX17004  2010      8   d5  29.6  15.8\n",
       "16       MX17004  2010      8  d29  28.0  15.3\n",
       "17       MX17004  2010      8  d13  29.8  16.5\n",
       "18       MX17004  2010      8  d25  29.7  15.6\n",
       "19       MX17004  2010      8  d31  25.4  15.4\n",
       "20       MX17004  2010      8   d8  29.0  17.3\n",
       "21       MX17004  2010     10   d5  27.0  14.0\n",
       "22       MX17004  2010     10  d14  29.5  13.0\n",
       "23       MX17004  2010     10  d15  28.7  10.5\n",
       "24       MX17004  2010     10  d28  31.2  15.0\n",
       "25       MX17004  2010     10   d7  28.1  12.9\n",
       "26       MX17004  2010     11   d2  31.3  16.3\n",
       "27       MX17004  2010     11   d5  26.3   7.9\n",
       "28       MX17004  2010     11  d27  27.7  14.2\n",
       "29       MX17004  2010     11  d26  28.1  12.1\n",
       "30       MX17004  2010     11   d4  27.2  12.0\n",
       "31       MX17004  2010     12   d1  29.9  13.8\n",
       "32       MX17004  2010     12   d6  27.8  10.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tidy data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pew = pd.read_csv('data/pew.csv')\n",
    "pew.head()\n",
    "\n",
    "pew_long = pd.melt(pew, id_vars='religion')\n",
    "pew_long.head()\n",
    "\n",
    "billboard = pd.read_csv('data/billboard.csv')\n",
    "billboard.head()\n",
    "\n",
    "billboard_melt = pd.melt(\n",
    "    billboard,\n",
    "    id_vars=['year', 'artist', 'track', 'time', 'date.entered'],\n",
    "    var_name='week',\n",
    "    value_name='rating'\n",
    ")\n",
    "billboard_melt.tail()\n",
    "\n",
    "# drop_thresh = billboard.shape[0]*0.9\n",
    "# df_n = billboard.dropna(thresh=drop_thresh, how='all', axis='columns').copy()\n",
    "# df_n.shape\n",
    "\n",
    "# weather = pd.read_csv('https://github.com/chendaniely/pandas_for_everyone/blob/master/data/weather.csv')\n",
    "weather = pd.read_csv('data/weather.csv')\n",
    "weather.head()\n",
    "\n",
    "weather_long = pd.melt(\n",
    "    weather,\n",
    "    id_vars=['id', 'year', 'month', 'element'],\n",
    "    var_name='day',\n",
    "    value_name='temp'\n",
    ")\n",
    "weather_long.head()\n",
    "\n",
    "weather_tidy = weather_long.pivot_table(\n",
    "    index=['id', 'year', 'month', 'day'],\n",
    "    columns='element',\n",
    "    values='temp'\n",
    ")\n",
    "weather_tidy.head(6)\n",
    "weather_tidy.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit: None\n",
      "python: 3.7.3.final.0\n",
      "python-bits: 64\n",
      "OS: Windows\n",
      "OS-release: 10\n",
      "machine: AMD64\n",
      "processor: Intel64 Family 6 Model 37 Stepping 2, GenuineIntel\n",
      "byteorder: little\n",
      "LC_ALL: None\n",
      "LANG: None\n",
      "LOCALE: None.None\n",
      "\n",
      "pandas: 0.24.2\n",
      "pytest: 4.3.1\n",
      "pip: 19.0.3\n",
      "setuptools: 40.8.0\n",
      "Cython: 0.29.6\n",
      "numpy: 1.16.2\n",
      "scipy: 1.2.1\n",
      "pyarrow: None\n",
      "xarray: None\n",
      "IPython: 7.4.0\n",
      "sphinx: 1.8.5\n",
      "patsy: 0.5.1\n",
      "dateutil: 2.8.0\n",
      "pytz: 2018.9\n",
      "blosc: None\n",
      "bottleneck: 1.2.1\n",
      "tables: 3.5.1\n",
      "numexpr: 2.6.9\n",
      "feather: None\n",
      "matplotlib: 3.0.3\n",
      "openpyxl: 2.6.1\n",
      "xlrd: 1.2.0\n",
      "xlwt: 1.3.0\n",
      "xlsxwriter: 1.1.5\n",
      "lxml.etree: 4.3.2\n",
      "bs4: 4.7.1\n",
      "html5lib: 1.0.1\n",
      "sqlalchemy: 1.3.1\n",
      "pymysql: 0.9.3\n",
      "psycopg2: None\n",
      "jinja2: 2.10\n",
      "s3fs: None\n",
      "fastparquet: None\n",
      "pandas_gbq: None\n",
      "pandas_datareader: None\n",
      "gcsfs: None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trantow-Barrows</td>\n",
       "      <td>Maintenance</td>\n",
       "      <td>2</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name      Product  Quantity   Status\n",
       "2  Trantow-Barrows  Maintenance         2  pending"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pd.show_versions()\n",
    "df = pd.read_csv(\"data/sales-funnel.csv\")\n",
    "df.head()\n",
    "df.columns\n",
    "df.index\n",
    "df.values\n",
    "type(df)\n",
    "df.shape  # df.shape[0]     df.shape[1]\n",
    "# df.info()\n",
    "\n",
    "name_df = df['Name']\n",
    "type(name_df)\n",
    "\n",
    "# .loc  - Access a group of rows and columns by label(s) or a boolean array. \n",
    "# .iloc  - Purely integer-location based indexing for selection by position.\n",
    "subset_col = df[['Name', 'Price']]   \n",
    "subset_col.head()\n",
    "\n",
    "subset_r_c = df.loc[ : , ['Name', 'Status', 'Price']]\n",
    "subset_r_c.head()\n",
    "subset_row_only = df.loc[[2, 0, 1]]\n",
    "subset_row_only.head()\n",
    "\n",
    "df.iloc[2]       # positional indexing\n",
    "\n",
    "df[\"Status\"] = df[\"Status\"].astype(\"category\")\n",
    "df[\"Status\"].cat.set_categories([\"won\",\"pending\",\"presented\",\"declined\"],inplace=True)\n",
    "# df[\"Status\"]\n",
    "\n",
    "# filtering\n",
    "df.loc[df['Name'] == 'Trantow-Barrows', ['Name','Product', 'Quantity','Status']]\n",
    "df.loc[ (df['Name'] == 'Trantow-Barrows') & (df['Quantity'] > 1), ['Name','Product', 'Quantity','Status']]\n",
    "\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change_Type</th>\n",
       "      <th>Covered_Recipient_Type</th>\n",
       "      <th>Recipient_Primary_Business_Street_Address_Line1</th>\n",
       "      <th>Recipient_City</th>\n",
       "      <th>Recipient_State</th>\n",
       "      <th>Recipient_Zip_Code</th>\n",
       "      <th>Recipient_Country</th>\n",
       "      <th>Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name</th>\n",
       "      <th>Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_ID</th>\n",
       "      <th>Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Total_Amount_of_Payment_USDollars</th>\n",
       "      <th>Date_of_Payment</th>\n",
       "      <th>Form_of_Payment_or_Transfer_of_Value</th>\n",
       "      <th>Preclinical_Research_Indicator</th>\n",
       "      <th>Delay_in_Publication_Indicator</th>\n",
       "      <th>Name_of_Study</th>\n",
       "      <th>Dispute_Status_for_Publication</th>\n",
       "      <th>Record_ID</th>\n",
       "      <th>Program_Year</th>\n",
       "      <th>Payment_Publication_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNCHANGED</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>PO BOX 365067</td>\n",
       "      <td>SAN JUAN</td>\n",
       "      <td>PR</td>\n",
       "      <td>00936</td>\n",
       "      <td>United States</td>\n",
       "      <td>ViiV Healthcare Company</td>\n",
       "      <td>100000005550</td>\n",
       "      <td>ViiV Healthcare Company</td>\n",
       "      <td>...</td>\n",
       "      <td>91.67</td>\n",
       "      <td>2013-09-20</td>\n",
       "      <td>Cash or cash equivalent</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A Phase IIIb randomized openlabel study of the...</td>\n",
       "      <td>No</td>\n",
       "      <td>106200494</td>\n",
       "      <td>2013</td>\n",
       "      <td>06/28/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNCHANGED</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>4290 Glendale Milford</td>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>OH</td>\n",
       "      <td>45242</td>\n",
       "      <td>United States</td>\n",
       "      <td>Merck Sharp &amp; Dohme Corporation</td>\n",
       "      <td>100000000053</td>\n",
       "      <td>Merck Sharp &amp; Dohme Corporation</td>\n",
       "      <td>...</td>\n",
       "      <td>1216.53</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>In-kind items and services</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A Phase II, Randomized, Active Comparator-Cont...</td>\n",
       "      <td>No</td>\n",
       "      <td>281346588</td>\n",
       "      <td>2013</td>\n",
       "      <td>06/28/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNCHANGED</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>100 UCLA MEDICAL PLZ STE 510</td>\n",
       "      <td>LOS ANGELES</td>\n",
       "      <td>CA</td>\n",
       "      <td>90095</td>\n",
       "      <td>United States</td>\n",
       "      <td>Merck Sharp &amp; Dohme Corporation</td>\n",
       "      <td>100000000053</td>\n",
       "      <td>Merck Sharp &amp; Dohme Corporation</td>\n",
       "      <td>...</td>\n",
       "      <td>15.22</td>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>In-kind items and services</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A Phase III Randomized, Placebo-Controlled, Cl...</td>\n",
       "      <td>No</td>\n",
       "      <td>281403760</td>\n",
       "      <td>2013</td>\n",
       "      <td>06/28/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNCHANGED</td>\n",
       "      <td>Covered Recipient Physician</td>\n",
       "      <td>340 GOLDEN SHORE  400</td>\n",
       "      <td>LONG BEACH</td>\n",
       "      <td>CA</td>\n",
       "      <td>90802</td>\n",
       "      <td>United States</td>\n",
       "      <td>Merck Sharp &amp; Dohme Corporation</td>\n",
       "      <td>100000000053</td>\n",
       "      <td>Merck Sharp &amp; Dohme Corporation</td>\n",
       "      <td>...</td>\n",
       "      <td>412.00</td>\n",
       "      <td>2013-11-08</td>\n",
       "      <td>In-kind items and services</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A Double BlindPlacebo Controlled Trial of Asen...</td>\n",
       "      <td>No</td>\n",
       "      <td>106069910</td>\n",
       "      <td>2013</td>\n",
       "      <td>06/28/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNCHANGED</td>\n",
       "      <td>Covered Recipient Teaching Hospital</td>\n",
       "      <td>111 E 210th St</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>NY</td>\n",
       "      <td>10467</td>\n",
       "      <td>United States</td>\n",
       "      <td>Pfizer Inc.</td>\n",
       "      <td>100000000286</td>\n",
       "      <td>Pfizer Inc.</td>\n",
       "      <td>...</td>\n",
       "      <td>180.92</td>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>In-kind items and services</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>ARCHER 1042  A PHASE 2 STUDY OF DACOMITINIB IN...</td>\n",
       "      <td>No</td>\n",
       "      <td>207498022</td>\n",
       "      <td>2013</td>\n",
       "      <td>06/28/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Change_Type               Covered_Recipient_Type  \\\n",
       "0   UNCHANGED          Covered Recipient Physician   \n",
       "1   UNCHANGED          Covered Recipient Physician   \n",
       "2   UNCHANGED          Covered Recipient Physician   \n",
       "3   UNCHANGED          Covered Recipient Physician   \n",
       "4   UNCHANGED  Covered Recipient Teaching Hospital   \n",
       "\n",
       "  Recipient_Primary_Business_Street_Address_Line1 Recipient_City  \\\n",
       "0                                   PO BOX 365067       SAN JUAN   \n",
       "1                           4290 Glendale Milford     Cincinnati   \n",
       "2                    100 UCLA MEDICAL PLZ STE 510    LOS ANGELES   \n",
       "3                           340 GOLDEN SHORE  400     LONG BEACH   \n",
       "4                                  111 E 210th St          Bronx   \n",
       "\n",
       "  Recipient_State Recipient_Zip_Code Recipient_Country  \\\n",
       "0              PR              00936     United States   \n",
       "1              OH              45242     United States   \n",
       "2              CA              90095     United States   \n",
       "3              CA              90802     United States   \n",
       "4              NY              10467     United States   \n",
       "\n",
       "  Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name  \\\n",
       "0                            ViiV Healthcare Company          \n",
       "1                    Merck Sharp & Dohme Corporation          \n",
       "2                    Merck Sharp & Dohme Corporation          \n",
       "3                    Merck Sharp & Dohme Corporation          \n",
       "4                                        Pfizer Inc.          \n",
       "\n",
       "   Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_ID  \\\n",
       "0                                       100000005550             \n",
       "1                                       100000000053             \n",
       "2                                       100000000053             \n",
       "3                                       100000000053             \n",
       "4                                       100000000286             \n",
       "\n",
       "  Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name  ...  \\\n",
       "0                            ViiV Healthcare Company             ...   \n",
       "1                    Merck Sharp & Dohme Corporation             ...   \n",
       "2                    Merck Sharp & Dohme Corporation             ...   \n",
       "3                    Merck Sharp & Dohme Corporation             ...   \n",
       "4                                        Pfizer Inc.             ...   \n",
       "\n",
       "  Total_Amount_of_Payment_USDollars Date_of_Payment  \\\n",
       "0                             91.67      2013-09-20   \n",
       "1                           1216.53      2013-10-08   \n",
       "2                             15.22      2013-09-06   \n",
       "3                            412.00      2013-11-08   \n",
       "4                            180.92      2013-11-12   \n",
       "\n",
       "  Form_of_Payment_or_Transfer_of_Value  Preclinical_Research_Indicator  \\\n",
       "0              Cash or cash equivalent                              No   \n",
       "1           In-kind items and services                              No   \n",
       "2           In-kind items and services                              No   \n",
       "3           In-kind items and services                              No   \n",
       "4           In-kind items and services                              No   \n",
       "\n",
       "  Delay_in_Publication_Indicator  \\\n",
       "0                             No   \n",
       "1                             No   \n",
       "2                             No   \n",
       "3                             No   \n",
       "4                             No   \n",
       "\n",
       "                                       Name_of_Study  \\\n",
       "0  A Phase IIIb randomized openlabel study of the...   \n",
       "1  A Phase II, Randomized, Active Comparator-Cont...   \n",
       "2  A Phase III Randomized, Placebo-Controlled, Cl...   \n",
       "3  A Double BlindPlacebo Controlled Trial of Asen...   \n",
       "4  ARCHER 1042  A PHASE 2 STUDY OF DACOMITINIB IN...   \n",
       "\n",
       "  Dispute_Status_for_Publication  Record_ID Program_Year  \\\n",
       "0                             No  106200494         2013   \n",
       "1                             No  281346588         2013   \n",
       "2                             No  281403760         2013   \n",
       "3                             No  106069910         2013   \n",
       "4                             No  207498022         2013   \n",
       "\n",
       "  Payment_Publication_Date  \n",
       "0               06/28/2019  \n",
       "1               06/28/2019  \n",
       "2               06/28/2019  \n",
       "3               06/28/2019  \n",
       "4               06/28/2019  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# low_memory=False parameter in order to surpress this warning:\n",
    "df_raw = pd.read_csv('data/OP_DTL_RSRCH_PGYR2013_P06282019.csv', low_memory=False)\n",
    "# df.shape[0]  # gives number of row count\n",
    "# df.shape[1]  # gives number of column count\n",
    "\n",
    "#quickly drop all the columns where at least 90% of the data is empty. might be handy for others as well.\n",
    "drop_thresh = df_raw.shape[0]*0.9\n",
    "df = df_raw.dropna(thresh=drop_thresh, how='all', axis='columns').copy()\n",
    "# df.head()\n",
    "\n",
    "#---------------------------------\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "def str_to_dt2(s):\n",
    "    try:\n",
    "        date_object = parser.parse(str(s))\n",
    "    except ValueError:\n",
    "        date_object = datetime.datetime(1970,1,1)\n",
    "    except:\n",
    "        date_object = datetime.datetime(1970,1,1)\n",
    "    return date_object\n",
    "\n",
    "df['Date_of_Payment'] = df.Date_of_Payment.apply(str_to_dt2)\n",
    "df.head()\n",
    "\n",
    "#---------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 ms ± 378 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# keep continued from the above cell \n",
    "\n",
    "# df_raw.info()\n",
    "# df.info()\n",
    "\n",
    "# function to create a dataframe showing all the unique values in a column.\n",
    "unique_counts = pd.DataFrame.from_records([(col, df[col].nunique()) for col in df.columns],\n",
    "                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\n",
    "# unique_counts\n",
    "\n",
    "cols_to_exclude = ['Program_Year', 'Date_of_Payment', 'Payment_Publication_Date']\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() < 600 and col not in cols_to_exclude:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# df.groupby('Form_of_Payment_or_Transfer_of_Value')['Total_Amount_of_Payment_USDollars'].sum().to_frame()\n",
    "df.groupby('Covered_Recipient_Type')['Total_Amount_of_Payment_USDollars'].sum().to_frame()\n",
    "\n",
    "import time\n",
    "\n",
    "# %timeit df_raw.groupby('Covered_Recipient_Type')['Total_Amount_of_Payment_USDollars'].sum().to_frame()\n",
    "\n",
    "# %timeit df.groupby('Covered_Recipient_Type')['Total_Amount_of_Payment_USDollars'].sum().to_frame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let’s build a simple dataframe with one ordered categorical variable that represents \n",
    "# the status of the customer. \n",
    "# Here, highlight some potential subtle errors when dealing with categorical values.\n",
    "\n",
    "sales_1 = [{'account': 'Jones LLC', 'Status': 'Gold', 'Jan': 150, 'Feb': 200, 'Mar': 140},\n",
    "         {'account': 'Alpha Co', 'Status': 'Gold', 'Jan': 200, 'Feb': 210, 'Mar': 215},\n",
    "         {'account': 'Blue Inc',  'Status': 'Silver', 'Jan': 50,  'Feb': 90,  'Mar': 95 }]\n",
    "df_1 = pd.DataFrame(sales_1)\n",
    "# print(df_1)\n",
    "status_type = CategoricalDtype(categories=['Silver', 'Gold'], ordered=True)\n",
    "df_1['Status'] = df_1['Status'].astype(status_type)\n",
    "df_1\n",
    "\n",
    "sales_2 = [{'account': 'Smith Co', 'Status': 'Silver', 'Jan': 100, 'Feb': 100, 'Mar': 70},\n",
    "         {'account': 'Bingo', 'Status': 'Bronze', 'Jan': 310, 'Feb': 65, 'Mar': 80}]\n",
    "df_2 = pd.DataFrame(sales_2)\n",
    "df_2['Status'] = df_2['Status'].astype(status_type)\n",
    "df_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manager</th>\n",
       "      <th>Rep</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Debra Henley</th>\n",
       "      <th>Craig Booker</th>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel Hilton</th>\n",
       "      <td>115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John Smith</th>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Fred Anderson</th>\n",
       "      <th>Cedric Moss</th>\n",
       "      <td>110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wendy Yule</th>\n",
       "      <td>177000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Price\n",
       "Manager       Rep                  \n",
       "Debra Henley  Craig Booker    80000\n",
       "              Daniel Hilton  115000\n",
       "              John Smith      40000\n",
       "Fred Anderson Cedric Moss    110000\n",
       "              Wendy Yule     177000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.pivot_table\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pd.__version__\n",
    "# pd.show_versions()\n",
    "\n",
    "df = pd.read_csv(\"data/sales-funnel.csv\")\n",
    "\n",
    "# For convenience sake, let’s define the status column as a category and set the order we want to view.\n",
    "df['Status'] = df['Status'].astype('category')\n",
    "df['Status'].cat.set_categories(['won', 'pending', 'presented', 'declined'], inplace=True)\n",
    "df.head()\n",
    "\n",
    "# pd.pivot_table(df,index=['Name'])\n",
    "# pd.pivot_table(df,index=[\"Name\",\"Rep\",\"Manager\"])\n",
    "pd.pivot_table(df,index=[\"Manager\",\"Rep\"])\n",
    "pd.pivot_table(df,index=[\"Manager\",\"Rep\"],values=[\"Price\"])\n",
    "pd.pivot_table(df,index=[\"Manager\",\"Rep\"],values=[\"Price\"],aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparing \"map\", \"applymap\" and \"apply\": Context Matters\n",
    "    \n",
    "First major difference: DEFINITION\n",
    "\n",
    "    map is defined on Series ONLY\n",
    "    applymap is defined on DataFrames ONLY\n",
    "    apply is defined on BOTH\n",
    "\n",
    "Second major difference: INPUT ARGUMENT\n",
    "\n",
    "    map accepts dicts, Series, or callable\n",
    "    applymap and apply accept callables only\n",
    "    \n",
    "Third major difference: BEHAVIOR\n",
    "\n",
    "    map is elementwise for Series\n",
    "    applymap is elementwise for DataFrames\n",
    "    apply also works elementwise but is suited to more complex operations and aggregation. \n",
    "        The behaviour and return value depends on the function.\n",
    "\n",
    "Fourth major difference (the most important one): USE CASE\n",
    "\n",
    "    map is meant for mapping values from one domain to another, \n",
    "        so is optimised for performance (e.g., df['A'].map({1:'a', 2:'b', 3:'c'}))\n",
    "    applymap is good for elementwise transformations across multiple rows/columns \n",
    "        (e.g., df[['A', 'B', 'C']].applymap(str.strip))\n",
    "    apply is for applying any function that cannot be vectorised \n",
    "        (e.g., df['sentences'].apply(nltk.sent_tokenize))\n",
    "        \n",
    "\n",
    "<Footnotes>\n",
    "\n",
    "map when passed a dictionary/Series will map elements based on the keys in that dictionary/Series. \n",
    "        Missing values will be recorded as NaN in the output.\n",
    "applymap in more recent versions has been optimised for some operations. \n",
    "        You will find applymap slightly faster than apply in some cases. \n",
    "        My suggestion is to test them both and use whatever works better.\n",
    "\n",
    "map is optimised for elementwise mappings and transformation. \n",
    "        Operations that involve dictionaries or Series will enable pandas to use \n",
    "        faster code paths for better performance.\n",
    "\n",
    "Series.apply returns a scalar for aggregating operations, Series otherwise. \n",
    "\n",
    "Similarly for DataFrame.apply. \n",
    "    Note that apply also has fastpaths when called with certain NumPy functions such as mean, sum, etc.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fox\n",
      "1    cow\n",
      "2    NaN\n",
      "3    dog\n",
      "dtype: object \n",
      "\n",
      "0     cub\n",
      "1    calf\n",
      "2     NaN\n",
      "3     NaN\n",
      "dtype: object \n",
      "\n",
      "0    I am a fox\n",
      "1    I am a cow\n",
      "2    I am a nan\n",
      "3    I am a dog\n",
      "dtype: object \n",
      "\n",
      "0    I am a fox\n",
      "1    I am a cow\n",
      "2           NaN\n",
      "3    I am a dog\n",
      "dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "s = pd.Series(['fox', 'cow', np.nan, 'dog'])\n",
    "print(s, '\\n')\n",
    "\n",
    "#- map accepts a dict or a Series. \n",
    "#  Values that are not found in the dict are converted to NaN, unless the dict has a default value \n",
    "s2 = s.map({'fox': 'cub', 'cow': 'calf'})\n",
    "print(s2, '\\n')\n",
    "\n",
    "#It also accepts a function:\n",
    "s3 = s.map('I am a {}'.format)\n",
    "print(s3, '\\n')\n",
    "\n",
    "s4 = s.map('I am a {}'.format, na_action='ignore')\n",
    "print(s4, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>carrier</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>flight</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>air_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>N508AS</td>\n",
       "      <td>145</td>\n",
       "      <td>PDX</td>\n",
       "      <td>ANC</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>US</td>\n",
       "      <td>N195UW</td>\n",
       "      <td>1830</td>\n",
       "      <td>SEA</td>\n",
       "      <td>CLT</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>N37422</td>\n",
       "      <td>1609</td>\n",
       "      <td>PDX</td>\n",
       "      <td>IAH</td>\n",
       "      <td>201.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>US</td>\n",
       "      <td>N547UW</td>\n",
       "      <td>466</td>\n",
       "      <td>PDX</td>\n",
       "      <td>CLT</td>\n",
       "      <td>251.0</td>\n",
       "      <td>2282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  dep_time  dep_delay  arr_time  arr_delay carrier tailnum  \\\n",
       "0  2014      1    1       1.0       96.0     235.0       70.0      AS  N508AS   \n",
       "1  2014      1    1       4.0       -6.0     738.0      -23.0      US  N195UW   \n",
       "2  2014      1    1       8.0       13.0     548.0       -4.0      UA  N37422   \n",
       "3  2014      1    1      28.0       -2.0     800.0      -23.0      US  N547UW   \n",
       "\n",
       "   flight origin dest  air_time  distance  hour  minute  \n",
       "0     145    PDX  ANC     194.0      1542   0.0     1.0  \n",
       "1    1830    SEA  CLT     252.0      2279   0.0     4.0  \n",
       "2    1609    PDX  IAH     201.0      1825   0.0     8.0  \n",
       "3     466    PDX  CLT     251.0      2282   0.0    28.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_flights = pd.read_csv('data/flights.csv')\n",
    "\n",
    "df_flights.head(4)\n",
    "# dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "# dummy_df_age.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'total number of missing values'\n",
      " 248 \n",
      "\n",
      "'column-wise null values'\n",
      " carrier      0\n",
      "tailnum    248\n",
      "origin       0\n",
      "dest         0\n",
      "dtype: int64 \n",
      "\n",
      "'after fillna()'\n",
      " 0 \n",
      "\n",
      "'count of categories'\n",
      " AS    62460\n",
      "WN    23355\n",
      "OO    18710\n",
      "DL    16716\n",
      "UA    16671\n",
      "AA     7586\n",
      "US     5946\n",
      "B6     3540\n",
      "VX     3272\n",
      "F9     2698\n",
      "HA     1095\n",
      "Name: carrier, dtype: int64 \n",
      "\n",
      "'count of distinct categories'\n",
      " 11 \n",
      "\n",
      "'count of categories'\n",
      " SEA    108714\n",
      "PDX     53335\n",
      "Name: origin, dtype: int64 \n",
      "\n",
      "   carrier tailnum origin dest\n",
      "0        2  N508AS    PDX  ANC\n",
      "1        9  N195UW    SEA  CLT\n",
      "2        8  N37422    PDX  IAH\n",
      "3        9  N547UW    PDX  CLT\n",
      "4        2  N762AS    SEA  ANC \n",
      "\n",
      "carrier    category\n",
      "tailnum      object\n",
      "origin     category\n",
      "dest         object\n",
      "dtype: object \n",
      "\n",
      "0-20 20-40 40-60 60-80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-20</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20-40</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40-60</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60-80</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  age_mean\n",
       "0   0-20      10.0\n",
       "1  20-40      30.0\n",
       "2  40-60      50.0\n",
       "3  60-80      70.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "# !cat data/flights.csv\n",
    "\n",
    "\n",
    "# df_flights = pd.read_csv('https://raw.githubusercontent.com/ismayc/pnwflights14/master/data/flights.csv')\n",
    "df_flights = pd.read_csv('data/flights.csv')\n",
    "\n",
    "# df_flights.head()\n",
    "# df_flights.info()\n",
    "\n",
    "# df_flights.boxplot('dep_time','origin',rot = 30,figsize=(5,6))\n",
    "cat_df_flights = df_flights.select_dtypes(include=['object']).copy()\n",
    "cat_df_flights.head()\n",
    "\n",
    "#total number of missing values\n",
    "print(\"'total number of missing values'\\n\", cat_df_flights.isnull().values.sum(), '\\n')\n",
    "\n",
    "\n",
    "#column-wise null values:\n",
    "print(\"'column-wise null values'\\n\", cat_df_flights.isnull().sum(), '\\n')\n",
    "\n",
    "\n",
    "#fillna() is handy for replacing null values as NAN.\n",
    "cat_df_flights = cat_df_flights.fillna(cat_df_flights['tailnum'].value_counts().index[0])\n",
    "print(\"'after fillna()'\\n\", cat_df_flights.isnull().values.sum(), '\\n')\n",
    "\n",
    "#categorical features\n",
    "print(\"'count of categories'\\n\", cat_df_flights['carrier'].value_counts(),'\\n')\n",
    "print(\"'count of distinct categories'\\n\", cat_df_flights['carrier'].value_counts().count(),'\\n')\n",
    "print(\"'count of categories'\\n\", cat_df_flights['origin'].value_counts(),'\\n')\n",
    "\n",
    "# #1. Indexing\n",
    "\n",
    "# df_flights.loc[3, 'tailnum']\n",
    "# df_flights.loc[:, 'tailnum'].head()\n",
    "# df_flights.iloc[1:6, 1:6]\n",
    "\n",
    "# df_flights.query('dep_time > 40 and distance < 2500 ').head()\n",
    "\n",
    "\n",
    "# Visual exploration\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar chart\n",
    "# carrier_count = cat_df_flights['carrier'].value_counts()\n",
    "# sns.set(style=\"darkgrid\")\n",
    "#     # sns.barplot(data, x, y, kwargs)\n",
    "# sns.barplot(carrier_count.index, carrier_count.values, alpha=0.7)\n",
    "# plt.title('Frequency Distribution of Carriers')\n",
    "# plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "# plt.xlabel('Carrier', fontsize=12)\n",
    "# plt.show()\n",
    "\n",
    "# Pie chart\n",
    "# labels = cat_df_flights['carrier'].astype('category').cat.categories.tolist()\n",
    "# counts = cat_df_flights['carrier'].value_counts()\n",
    "# sizes = [counts[var_cat] for var_cat in labels]\n",
    "# fig1, ax1 = plt.subplots()\n",
    "# ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\n",
    "# ax1.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "# Encoding Categorical Data\n",
    "# - encode the categorical features to numeric quantities. \n",
    "\n",
    "#1. Replace Values\n",
    "# -- hard coded dictionary -- is easy for small number of categories --\n",
    "# replace_map = {'carrier': {'AA': 1, 'AS': 2, 'B6': 3, 'DL': 4,\n",
    "#                                   'F9': 5, 'HA': 6, 'OO': 7 , 'UA': 8 , 'US': 9,'VX': 10,'WN': 11}}\n",
    "\n",
    "# -- dictionary comprehensions -- for big number of categories \n",
    "labels = cat_df_flights['carrier'].astype('category').cat.categories.tolist()\n",
    "replace_map_comp = {'carrier' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
    "# print(replace_map_comp)\n",
    "\n",
    "cat_df_flights_replace = cat_df_flights.copy()\n",
    "cat_df_flights_replace.replace(replace_map_comp, inplace=True)\n",
    "\n",
    "print(cat_df_flights_replace.head(),'\\n')\n",
    "\n",
    "#-- typecasting by using .astype() method on your columns.\n",
    "# \" category dtype\" makes the operations on such columns much faster than the \"object dtype\".\n",
    "cat_df_flights_lc = cat_df_flights.copy()\n",
    "cat_df_flights_lc['carrier'] = cat_df_flights_lc['carrier'].astype('category')\n",
    "cat_df_flights_lc['origin'] = cat_df_flights_lc['origin'].astype('category')                                                              \n",
    "\n",
    "print(cat_df_flights_lc.dtypes,'\\n')\n",
    "\n",
    "import time\n",
    "#-- calculate the number of flights for each carrier from each origin places.\n",
    "cat_df_flights.groupby(['origin','carrier']).count()\n",
    "# %timeit cat_df_flights.groupby(['origin','carrier']).count() #DataFrame with object dtype columns\n",
    "cat_df_flights.groupby(['origin','carrier']).count()\n",
    "# %timeit cat_df_flights_lc.groupby(['origin','carrier']).count() #DataFrame with category dtype columns\n",
    "\n",
    "\n",
    "\n",
    "# Label Encoding\n",
    "cat_df_flights_lc.head()\n",
    "cat_df_flights_lc['carrier'] = cat_df_flights_lc['carrier'].cat.codes\n",
    "cat_df_flights_lc.head()\n",
    "\n",
    "#- encode all the US carrier flights to value 1 and other carriers to value 0. \n",
    "cat_df_flights_specific = cat_df_flights.copy()\n",
    "cat_df_flights_specific['US_code'] = np.where(cat_df_flights_specific['carrier'].str.contains('US'), 1, 0)\n",
    "\n",
    "cat_df_flights_specific.head()\n",
    "\n",
    "#- create dummy DataFrame which has just one feature \"age\" with ranges specified.\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "\n",
    "# Series.map()\n",
    "print(*dummy_df_age['age'])\n",
    "dummy_df_age['start'], dummy_df_age['end'] = zip(*dummy_df_age['age'].map(lambda x: x.split('-')))\n",
    "\n",
    "dummy_df_age.head()\n",
    "\n",
    "\n",
    "# Seriese( or DataFrame ).apply()\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "\n",
    "def split_mean(x):\n",
    "    split_list = x.split('-')\n",
    "    mean = (float(split_list[0])+float(split_list[1]))/2\n",
    "    return mean\n",
    "\n",
    "dummy_df_age['age_mean'] = dummy_df_age['age'].apply(lambda x: split_mean(x))\n",
    "\n",
    "dummy_df_age.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pandas dtype       Python type        NumPy type                        Usage\n",
    "\n",
    "object                     str                         string_, unicode_              Text\n",
    "int64                      int                         int_, int8, int16, int32,     Integer numbers\n",
    "                                                              int64, uint8, uint16, \n",
    "                                                              uint32, uint64\n",
    "float64                  float                       float_, float16, float32,     Floating point numbers\n",
    "                                                              float64\n",
    "bool                       bool                       bool_                                    True/False values\n",
    "datetime64          NA                         datetime64[ns]                   Date and time values\n",
    "timedelta[ns]      NA                         NA                                          Differences between two datetimes\n",
    "category               NA                          NA                                         Finite list of text values\n",
    "\n",
    "Categorical data example, t-shirt, \n",
    "it could have categorical variables such as:\n",
    "\n",
    "    Size (X-Small, Small, Medium, Large, X-Large)\n",
    "    Color (Red, Black, White)\n",
    "    Style (Short sleeve, long sleeve)\n",
    "    Material (Cotton, Polyester)\n",
    "    \n",
    "Attributes such as cost, price, quantity are typically integers or floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical without pandas.Categorical() : \n",
      " 0    a\n",
      "1    a\n",
      "2    a\n",
      "3    b\n",
      "4    d\n",
      "5    a\n",
      "6    d\n",
      "7    c\n",
      "dtype: category\n",
      "Categories (4, object): [a, b, c, d]\n",
      "\n",
      "\n",
      "c1 :  [1, 2, 3, 1, 2, 3, 3, 3]\n",
      "Categories (3, int64): [1, 2, 3]\n",
      "\n",
      "c2 :  [e, m, f, i, f, e, h, m]\n",
      "Categories (5, object): [e, f, h, i, m]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "  \n",
    "# Categorical using dtype \n",
    "c = pd.Series([\"a\",\"a\",\"a\", \"b\", \"d\", \"a\", \"d\",\"c\"], dtype =\"category\") \n",
    "print (\"\\nCategorical without pandas.Categorical() : \\n\", c) \n",
    "  \n",
    "  \n",
    "c1 = pd.Categorical([1, 2, 3, 1, 2, 3, 3, 3]) \n",
    "print (\"\\n\\nc1 : \", c1) \n",
    "  \n",
    "c2 = pd.Categorical(['e', 'm', 'f', 'i', 'f', 'e', 'h', 'm' ]) \n",
    "print (\"\\nc2 : \", c2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,\n",
      ",,\n",
      "col_one,col_two,col_three\n",
      ",,\n",
      ",,\n",
      "10,20,30\n",
      "40,50,60\n",
      "70,80,90\n",
      "1.36 ms ± 1.31 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.2 ms ± 30.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!cat data/data.csv\n",
    "\n",
    "df = pd.read_csv('data/data.csv', header=2, skiprows=[3, 4])\n",
    "# df.info()\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "df.head()\n",
    "%timeit df.groupby('Age')['Survived'].sum()\n",
    "\n",
    "df['Age'] = df['Age'].astype('category')\n",
    "%timeit df.groupby('Age')['Survived'].sum()\n",
    "\n",
    "# Two easy ways to reduce DataFrame memory usage:\n",
    "# 1. Only read in columns you need\n",
    "# 2. Use 'category' data type with categorical data.\n",
    "# df = pd.read _csv('data/data.csv', usecols=['col_one', 'col_two', 'col_three'], dtype={'col_three':'category'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First the 'pandas' library, a very powerful library\n",
    "# used by data scientist and analysts to manipulate, preprocess data.\n",
    "# Through pandas, you get acquainted with your data \n",
    "# by cleaning, transforming, and analyzing it.\n",
    "#\n",
    "# Seaborn is a library for making statistical graphics in Python. \n",
    "# It is built on top of matplotlib and closely integrated with pandas data structures.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Next we're going to import the data file. \n",
    "#   pd.read_csv   for csv files.\n",
    "#  read_excel   for xls files. \n",
    "# This will import all your data and create a dataframe. \n",
    "# If an index is not specified it will insert an index starting at 0, \n",
    "# however you can adjust the index to start wherever you want or use a column name as an index\n",
    "df = pd.read_excel('data.xls')\n",
    "\n",
    "# Next you will notice that price prints in scientific notation (float64) so we have to convert it to (int64)\n",
    "df['price']=df['price'].astype('int64')\n",
    "df\n",
    "\n",
    "# TRANSFORMATION #1:\n",
    "# Let's view some statistics on this dataset.\n",
    "df.describe()\n",
    "\n",
    "# TRANSFORMATION #2:\n",
    "# Now lets derive a calculated field called price per sq ft in excel and in pandas \n",
    "# and sort by the derived field in descending order\n",
    "df['price_sqft'] = df['price']/df['sqft_lot']\n",
    "df = df.sort_values(by=['price_sqft'], ascending=False)\n",
    "df.head()\n",
    "\n",
    "# TRANSFORMATION #3:\n",
    "# Find the count of sales and mean price in each city by bedroom. \n",
    "# We will accomplish this by plotting a pivot table\n",
    "dfpiv = pd.pivot_table(df,index=['city'], \n",
    "                      values=['price', 'price'], \n",
    "                      columns=['bedrooms'], \n",
    "                      aggfunc=['count','mean'], \n",
    "                      fill_value=0)\n",
    "dfpiv.head()\n",
    "\n",
    "# TRANSFORMATION #4:\n",
    "# Now let's append the average 5 year appreciation rate by city \n",
    "# using a vlookup in excel and similar in pandas\n",
    "df_appre = pd.read_excel('data.xls', sheet_name='city_AA')\n",
    "results = df.merge(df_appre, on='city')\n",
    "results.head()\n",
    "\n",
    "results.loc[results['city']=='Medina'].head()\n",
    "\n",
    "\n",
    "# TRANSFORMATION #5:\n",
    "# Plot the average price per sqft by bedroom using a bar graph\n",
    "graph = sns.barplot(x='bedrooms', y='price_sqft', data=results, palette='Blues_d', errwidth=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       apples  oranges\n",
      "David       1        2\n",
      "June        3        0\n",
      "        apples  oranges\n",
      "Lily         0        7\n",
      "Robert       2        3\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4 entries, David to Robert\n",
      "Data columns (total 2 columns):\n",
      "apples     4 non-null int64\n",
      "oranges    4 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 96.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLES</th>\n",
       "      <th>ORANGES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>David</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>June</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lily</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        APPLES  ORANGES\n",
       "David    False    False\n",
       "June     False    False\n",
       "Lily     False    False\n",
       "Robert   False    False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The primary two components of pandas are the Series and DataFrame.\n",
    "# Series = essentially a column,\n",
    "# DataFrame = a multi-dimensional table made up of a collection of Series.\n",
    "\n",
    "data = {\n",
    "    'apples': [3, 2, 0, 1], \n",
    "    'oranges': [0, 3, 7, 2]\n",
    "}\n",
    "\n",
    "purchases = pd.DataFrame(data)\n",
    "# print(purchases)\n",
    "\n",
    "# Each (key, value) item in \"data\" corresponds to a column in the resulting DataFrame.\n",
    "\n",
    "# The Index of this DataFrame was given to us on creation as the default numbers 0-3,\n",
    "#  but we could also create our own when we initialize the DataFrame.\n",
    "\n",
    "purchases = pd.DataFrame(data, index=['June', 'Robert', 'Lily', 'David'])\n",
    "# print(purchases)\n",
    "\n",
    "# So now we could locate a customer's order by using their name:\n",
    "purchases.loc['June']\n",
    "\n",
    "\n",
    "###### How to read in data #######\n",
    "\n",
    "# df = pd.read_csv('data3.csv')\n",
    "# df.set_index('Responder_id')\n",
    "df = pd.read_csv('purchases.csv', index_col=0)\n",
    "# print(df)\n",
    "\n",
    "df = pd.read_json('purchases.json')\n",
    "# print(df.head(2))\n",
    "# print(df.tail(2))\n",
    "\n",
    "df.info()\n",
    "\n",
    "# .shape has no parentheses and returns a simple tuple of format (rows, columns)\n",
    "df.shape\n",
    "\n",
    "# append() will return a copy without affecting the original DataFrame. \n",
    "temp_df = df.append(df)\n",
    "temp_df.shape\n",
    "temp_df\n",
    "\n",
    "# try dropping duplicates from original df:\n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "temp_df.shape\n",
    "\n",
    "# how to print the column names of our dataset:\n",
    "df.columns\n",
    "\n",
    "# .rename() method to rename certain or all columns via a dict\n",
    "temp_df.rename(columns={\n",
    "        'aid': 'e_id', \n",
    "        'firstName': 'first_name'\n",
    "    }, inplace=True)\n",
    "\n",
    "temp_df.columns\n",
    "\n",
    "# Instead of just renaming each column manually we can do a list comprehension:\n",
    "temp_df.columns = [col.upper() for col in temp_df]\n",
    "temp_df.columns\n",
    "\n",
    "temp_df.isnull()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action,Adventure,Sci-Fi       50\n",
       "Drama                         48\n",
       "Comedy,Drama,Romance          35\n",
       "Comedy                        32\n",
       "Drama,Romance                 31\n",
       "Animation,Adventure,Comedy    27\n",
       "Comedy,Drama                  27\n",
       "Action,Adventure,Fantasy      27\n",
       "Comedy,Romance                26\n",
       "Crime,Drama,Thriller          24\n",
       "Crime,Drama,Mystery           23\n",
       "Action,Adventure,Drama        18\n",
       "Action,Crime,Drama            17\n",
       "Horror,Thriller               16\n",
       "Drama,Thriller                15\n",
       "Adventure,Family,Fantasy      14\n",
       "Biography,Drama               14\n",
       "Biography,Drama,History       14\n",
       "Action,Adventure,Comedy       14\n",
       "Action,Comedy,Crime           12\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# CSVs don't have indexes like our DataFrames, \n",
    "# so all we need to do is just designate the index_col when reading\n",
    "\n",
    "# df = pd.read_csv('Movie.csv', index_col=0)      # pd.read_json # pd.read_excel\n",
    "df = pd.read_csv('Movie.csv', index_col=\"Title\")\n",
    "df.head()\n",
    "df.tail(2)\n",
    "\n",
    "\n",
    "# Reading data from a SQL database\n",
    "# import sqlite3\n",
    "# con = sqlite3.connect(\"database.db\")\n",
    "# df = pd.read_sql_query(\"SELECT * FROM purchases\", con)\n",
    "\n",
    "# Converting back to a CSV, JSON, or SQL\n",
    "# df.to_csv('new_purchases.csv')\n",
    "# df.to_json('new_purchases.json')\n",
    "# df.to_sql('new_purchases', con)\n",
    "\n",
    "temp_df = df.append(df)\n",
    "# temp_df.shape\n",
    "# temp_df.tail(2)\n",
    "\n",
    "# temp_df = temp_df.drop_duplicates()\n",
    "# temp_df.shape\n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "temp_df.shape\n",
    "\n",
    "# df.info()\n",
    "# df.shape                # outputs just a tuple of (rows, columns)\n",
    "# df.dtypes\n",
    "# df.isnull()\n",
    "# df.isnull().sum()\n",
    "\n",
    "df.rename(columns={\n",
    "        'Runtime (Minutes)': 'Runtime', \n",
    "        'Revenue (Millions)': 'Revenue_millions'    }, inplace=True)\n",
    "df.columns\n",
    "\n",
    "df.columns = [col.lower() for col in df]\n",
    "df.columns\n",
    "\n",
    "df.isnull().sum()\n",
    "# delete any row with at least a single null value, but it will return a new DataFrame \n",
    "# without altering the original one.\n",
    "df.dropna()\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "revenue = df['revenue_millions']\n",
    "revenue.head()\n",
    "revenue_mean = revenue.mean()\n",
    "revenue_mean\n",
    "revenue.fillna(revenue_mean, inplace=True)\n",
    "df.isnull().sum()\n",
    "\n",
    "df.describe()\n",
    "df['genre'].describe()\n",
    "df['genre'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrival</th>\n",
       "      <th>ATL</th>\n",
       "      <th>DFW</th>\n",
       "      <th>ORD</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>departure</th>\n",
       "      <th>airlines</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JFK</th>\n",
       "      <th>SouthWest</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LAX</th>\n",
       "      <th>AA</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SFO</th>\n",
       "      <th>Delta</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean Air</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <th></th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "arrival               ATL  DFW  ORD  All\n",
       "departure airlines                      \n",
       "JFK       SouthWest     1    0    0    1\n",
       "LAX       AA            1    0    0    1\n",
       "          Delta         0    1    0    1\n",
       "SFO       Delta         0    0    2    2\n",
       "          Korean Air    0    1    0    1\n",
       "All                     2    2    2    6"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Styling\n",
    "\n",
    "# - Highlight all negative values in a dataframe.\n",
    "def set_color(val):\n",
    "    color = 'red' if val < 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "df = pd.DataFrame(dict(col_1=[1.53, -2.5, 3.53],\n",
    "                                             col_2=[-4.1, 5.9, 0])\n",
    "                                   )\n",
    "df.style.applymap(set_color)\n",
    "\n",
    "# - Hide the index.\n",
    "df.head().style.hide_index()\n",
    "\n",
    "#- Add hovering effects.\n",
    "df = pd.DataFrame(np.random.randn(5, 3))\n",
    "df.style.set_table_styles(\n",
    "    [{'selector': 'tr:hover',\n",
    "      'props': [('background-color', 'yellow')]}]\n",
    ")\n",
    "\n",
    "#- More CSS styles. You can use CSS to change the appearance of the table.\n",
    "df = pd.DataFrame(\n",
    "                dict(departure=['SFO', 'SFO', 'LAX', 'LAX', 'JFK', 'SFO'],\n",
    "                         arrival=['ORD', 'DFW', 'DFW', 'ATL', 'ATL', 'ORD'],\n",
    "                         airlines=['Delta','JetBlue','Delta','AA','SouthWest', 'Delta']),\n",
    "                     columns=['airlines', 'departure','arrival'])\n",
    "\n",
    "df.style.set_table_styles(\n",
    "            [{'selector': 'tr:nth-of-type(odd)', 'props': [('background', '#eee')]}, \n",
    "             {'selector': 'tr:nth-of-type(even)', 'props': [('background', 'white')]},\n",
    "             {'selector': 'th',   'props': [('background', '#606060'), \n",
    "                                                            ('color', 'white'),\n",
    "                                                            ('font-family', 'verdana')]},\n",
    "             {'selector': 'td',\n",
    "              'props': [('font-family', 'verdana')]},\n",
    "            ]\n",
    ").hide_index()\n",
    "\n",
    "\n",
    "# 2. Pandas options\n",
    "\"\"\"\n",
    "(1) There’re too many columns / rows in the dataframe and some columns / rows in the middle are omitted.\n",
    "(2) Columns containing long texts get truncated.\n",
    "(3) Columns containing floats display too many / too few digits.\n",
    "\"\"\"\n",
    "pd.options.display.max_columns = 50  # None -> No Restrictions\n",
    "pd.options.display.max_rows = 200    # None -> Be careful with this \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "# 3. Group by with multiple aggregations\n",
    "df = pd.DataFrame(dict(A=['coke', 'sprite', 'coke', 'sprite', 'sprite', 'coke', 'coke'],\n",
    "                                             B=['alpha','gamma', 'alpha', 'beta',  'gamma', 'beta', 'beta'],\n",
    "                                             col_1=[1,2,3,4,5,6,7],\n",
    "                                             col_2=[1,6,2,4,7,9,3]))\n",
    "\n",
    "tbl = df.groupby(['A','B']).agg({'col_1': ['max', np.mean],\n",
    "                                                         'col_2': ['sum','min','count']})\n",
    "tbl\n",
    "# 'count' will always be the count for number of rows in each group.\n",
    "\n",
    "\n",
    "# 4. Column slicing\n",
    "df.iloc[:,2:5].head()             # select the 2nd to the 4th column\n",
    "df.loc[:,'B':].head()   # select all columns starting from column 'B'\n",
    "\n",
    "\n",
    "# 5. Add row ID / random row ID to each group\n",
    "\n",
    "# 6. List all unique values in a group\n",
    "df = pd.DataFrame(dict(A=['A','A','A','A','A','B','B','B','B'],\n",
    "                                             B=[1,1,1,2,2,1,1,1,2],\n",
    "                                             C=['CA','NY','CA','FL','FL', 'WA','FL','NY','WA']))\n",
    "\n",
    "tbl = df[['A', 'B', 'C']].drop_duplicates()\\\n",
    "                         .groupby(['A','B'])['C']\\\n",
    "                         .apply(list)\\\n",
    "                         .reset_index()\n",
    "\n",
    "# list to string (separated by commas) \n",
    "tbl['C'] = tbl.apply(lambda x: (','.join([str(s) for s in x['C']])), axis = 1)\n",
    "#             \"\"\"\n",
    "#             Axis 0 will act on all the ROWS in each COLUMN\n",
    "#             Axis 1 will act on all the COLUMNS in each ROW\n",
    "#             \"\"\"\n",
    "tbl\n",
    "\n",
    "# 7. Add row total and column total to a numerical dataframe\n",
    "df = pd.DataFrame(dict(A=[2,6,3],\n",
    "                                             B=[2,2,6], \n",
    "                                             C=[3,2,3]))\n",
    "\n",
    "df['col_total']     = df.apply(lambda x: x.sum(), axis=1)\n",
    "df.loc['row_total'] = df.apply(lambda x: x.sum())\n",
    "df\n",
    "\n",
    "# 8. Check memory usage\n",
    "#   .memory_usage(deep=True) can be used on Pandas dataframes to see \n",
    "# the amount of memory used (in bytes) for each column. \n",
    "# It’s useful when building machine learning models which may require a lot memory in training.\n",
    "\n",
    "# 9. Cumulative sum\n",
    "# From time to time, cumulative sum is required when you generate some statistical outcomes. \n",
    "# Simply do df['cumulative_sum'] = df['target_column'].cumsum() .\n",
    "\n",
    "# 10. Crosstab\n",
    "# When you need to count the frequencies for groups formed by 3+ features, \n",
    "# pd.crosstab() can make your life easier.\n",
    "df = pd.DataFrame(dict(departure=['SFO', 'SFO', 'LAX', 'LAX', 'JFK', 'SFO'],\n",
    "                                              arrival=['ORD', 'DFW', 'DFW', 'ATL', 'ATL', 'ORD'],\n",
    "                                              airlines=['Delta', 'Korean Air', 'Delta', 'AA', 'SouthWest', 'Delta']))\n",
    "df\n",
    "\n",
    "pd.crosstab(index=[df['departure'], df['airlines']],\n",
    "                       columns=[df['arrival']],\n",
    "                       rownames=['departure', 'airlines'],\n",
    "                       colnames=['arrival'],\n",
    "                       margins=True      # add subtotal \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Runtime (Minutes)</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Revenue (Millions)</th>\n",
       "      <th>Metascore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Prometheus</td>\n",
       "      <td>Adventure,Mystery,Sci-Fi</td>\n",
       "      <td>Following clues to the origin of mankind, a te...</td>\n",
       "      <td>Ridley Scott</td>\n",
       "      <td>Noomi Rapace, Logan Marshall-Green, Michael Fa...</td>\n",
       "      <td>2012</td>\n",
       "      <td>124</td>\n",
       "      <td>7.0</td>\n",
       "      <td>485820</td>\n",
       "      <td>126.46</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Split</td>\n",
       "      <td>Horror,Thriller</td>\n",
       "      <td>Three girls are kidnapped by a man with a diag...</td>\n",
       "      <td>M. Night Shyamalan</td>\n",
       "      <td>James McAvoy, Anya Taylor-Joy, Haley Lu Richar...</td>\n",
       "      <td>2016</td>\n",
       "      <td>117</td>\n",
       "      <td>7.3</td>\n",
       "      <td>157606</td>\n",
       "      <td>138.12</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sing</td>\n",
       "      <td>Animation,Comedy,Family</td>\n",
       "      <td>In a city of humanoid animals, a hustling thea...</td>\n",
       "      <td>Christophe Lourdelet</td>\n",
       "      <td>Matthew McConaughey,Reese Witherspoon, Seth Ma...</td>\n",
       "      <td>2016</td>\n",
       "      <td>108</td>\n",
       "      <td>7.2</td>\n",
       "      <td>60545</td>\n",
       "      <td>270.32</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Suicide Squad</td>\n",
       "      <td>Action,Adventure,Fantasy</td>\n",
       "      <td>A secret government agency recruits some of th...</td>\n",
       "      <td>David Ayer</td>\n",
       "      <td>Will Smith, Jared Leto, Margot Robbie, Viola D...</td>\n",
       "      <td>2016</td>\n",
       "      <td>123</td>\n",
       "      <td>6.2</td>\n",
       "      <td>393727</td>\n",
       "      <td>325.02</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The Great Wall</td>\n",
       "      <td>Action,Adventure,Fantasy</td>\n",
       "      <td>European mercenaries searching for black powde...</td>\n",
       "      <td>Yimou Zhang</td>\n",
       "      <td>Matt Damon, Tian Jing, Willem Dafoe, Andy Lau</td>\n",
       "      <td>2016</td>\n",
       "      <td>103</td>\n",
       "      <td>6.1</td>\n",
       "      <td>56036</td>\n",
       "      <td>45.13</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank           Title                     Genre  \\\n",
       "1     2      Prometheus  Adventure,Mystery,Sci-Fi   \n",
       "2     3           Split           Horror,Thriller   \n",
       "3     4            Sing   Animation,Comedy,Family   \n",
       "4     5   Suicide Squad  Action,Adventure,Fantasy   \n",
       "5     6  The Great Wall  Action,Adventure,Fantasy   \n",
       "\n",
       "                                         Description              Director  \\\n",
       "1  Following clues to the origin of mankind, a te...          Ridley Scott   \n",
       "2  Three girls are kidnapped by a man with a diag...    M. Night Shyamalan   \n",
       "3  In a city of humanoid animals, a hustling thea...  Christophe Lourdelet   \n",
       "4  A secret government agency recruits some of th...            David Ayer   \n",
       "5  European mercenaries searching for black powde...           Yimou Zhang   \n",
       "\n",
       "                                              Actors  Year  Runtime (Minutes)  \\\n",
       "1  Noomi Rapace, Logan Marshall-Green, Michael Fa...  2012                124   \n",
       "2  James McAvoy, Anya Taylor-Joy, Haley Lu Richar...  2016                117   \n",
       "3  Matthew McConaughey,Reese Witherspoon, Seth Ma...  2016                108   \n",
       "4  Will Smith, Jared Leto, Margot Robbie, Viola D...  2016                123   \n",
       "5      Matt Damon, Tian Jing, Willem Dafoe, Andy Lau  2016                103   \n",
       "\n",
       "   Rating   Votes  Revenue (Millions)  Metascore  \n",
       "1     7.0  485820              126.46       65.0  \n",
       "2     7.3  157606              138.12       62.0  \n",
       "3     7.2   60545              270.32       59.0  \n",
       "4     6.2  393727              325.02       40.0  \n",
       "5     6.1   56036               45.13       42.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__\n",
    "# pd.show_versions()\n",
    "# movies =  pd.read_excel('data.xls')\n",
    "movies =  pd.read_csv('Movie.csv')\n",
    "movies.head()\n",
    "movies.Genre.unique()\n",
    "\n",
    "movies[\n",
    "    (movies.Genre == 'Adventure') |\n",
    "    (movies.Genre == 'Sci-Fi') |\n",
    "    (movies.Genre == 'History') ].head()\n",
    "\n",
    "movies[movies.Genre.isin([ 'Action,Adventure,Sci-Fi', 'Sci-Fi', 'History']) ].head()\n",
    "movies[~movies.Genre.isin(['Action,Adventure,Sci-Fi', 'Sci-Fi', 'History']) ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow0_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 82.3%, transparent 82.3%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow1_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 99.8%, transparent 99.8%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow2_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 94.0%, transparent 94.0%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow3_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 83.8%, transparent 83.8%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow4_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 100.0%, transparent 100.0%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow5_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 94.6%, transparent 94.6%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow6_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 88.3%, transparent 88.3%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow7_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 77.1%, transparent 77.1%);\n",
       "        }    #T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow8_col2 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg, transparent 50.0%, lightblue 50.0%, lightblue 99.2%, transparent 99.2%);\n",
       "        }</style><table id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfc\" ><caption>stock Prices from October 2016</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Date</th>        <th class=\"col_heading level0 col1\" >Close</th>        <th class=\"col_heading level0 col2\" >Volume</th>        <th class=\"col_heading level0 col3\" >Symbol</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow0_col0\" class=\"data row0 col0\" >2016-10-03</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow0_col1\" class=\"data row0 col1\" >31.5</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow0_col2\" class=\"data row0 col2\" >14070500</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow0_col3\" class=\"data row0 col3\" > CSCO</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow1_col0\" class=\"data row1 col0\" >2016-10-03</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow1_col1\" class=\"data row1 col1\" >112.52</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow1_col2\" class=\"data row1 col2\" >21701800</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow1_col3\" class=\"data row1 col3\" > AAPL</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow2_col0\" class=\"data row2 col0\" >2016-10-03</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow2_col1\" class=\"data row2 col1\" >57.42</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow2_col2\" class=\"data row2 col2\" >19189500</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow2_col3\" class=\"data row2 col3\" > MSFT</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow3_col0\" class=\"data row3 col0\" >2016-10-04</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow3_col1\" class=\"data row3 col1\" >50.64</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow3_col2\" class=\"data row3 col2\" >14726400</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow3_col3\" class=\"data row3 col3\" > CSCO</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow4_col0\" class=\"data row4 col0\" >2016-10-04</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow4_col1\" class=\"data row4 col1\" >131.59</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow4_col2\" class=\"data row4 col2\" >21808600</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow4_col3\" class=\"data row4 col3\" > AAPL</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow5_col0\" class=\"data row5 col0\" >2016-10-04</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow5_col1\" class=\"data row5 col1\" >53.05</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow5_col2\" class=\"data row5 col2\" >19453100</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow5_col3\" class=\"data row5 col3\" > MSFT</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow6_col0\" class=\"data row6 col0\" >2016-10-05</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow6_col1\" class=\"data row6 col1\" >57.64</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow6_col2\" class=\"data row6 col2\" >16726400</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow6_col3\" class=\"data row6 col3\" > MSFT</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow7_col0\" class=\"data row7 col0\" >2016-10-05</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow7_col1\" class=\"data row7 col1\" >31.59</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow7_col2\" class=\"data row7 col2\" >11808600</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow7_col3\" class=\"data row7 col3\" > CSCO</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow8_col0\" class=\"data row8 col0\" >2016-10-05</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow8_col1\" class=\"data row8 col1\" >113.05</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow8_col2\" class=\"data row8 col2\" >21453100</td>\n",
       "                        <td id=\"T_6e297eb4_13b0_11ea_8cef_70f1a1b95dfcrow8_col3\" class=\"data row8 col3\" > AAPL</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x237b3bca390>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "# pd.__version__\n",
    "# pd.show_versions()\n",
    "\n",
    "#2. Create an DataFrame\n",
    "df = pd.DataFrame(\n",
    "        {\n",
    "            'col one':[100,200],\n",
    "            'col two':[300,400]\n",
    "        }\n",
    ")\n",
    "\n",
    "df\n",
    "#----------------------------\n",
    "pd.DataFrame(np.random.rand(4,8))\n",
    "\n",
    "pd.DataFrame(np.random.rand(4,8), columns=list('abcdefgh'))\n",
    "\n",
    "#3. Rename columns with 3 different ways\n",
    "df = df.rename({'col one':'col_one', 'col two':'col_two'}, axis='columns')\n",
    "df.columns = ['col_one', 'col_two']\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "df\n",
    "\n",
    "df.add_prefix('X_')\n",
    "df.add_suffix('_Y')\n",
    "df\n",
    "\n",
    "#4. Reverse row order\n",
    "# movies =  pd.read_excel('data.xls')\n",
    "movies =  pd.read_csv('Movie.csv')\n",
    "movies.head()\n",
    "movies.loc[::-1].head()\n",
    "movies.loc[::-1].head().reset_index(drop=True).head()\n",
    "\n",
    "#5. Reverse column order\n",
    "movies.loc[:, ::-1].head()\n",
    "\n",
    "#6. Select columns by data type\n",
    "movies.dtypes\n",
    "movies.select_dtypes(include='number').head()\n",
    "movies.select_dtypes(include='object').head()\n",
    "movies.select_dtypes(include=['object', 'category', 'datetime']).head()\n",
    "movies.select_dtypes(exclude='number').head()\n",
    "\n",
    "#7. Convert strings to numbers\n",
    "df = pd.DataFrame(\n",
    "        {\n",
    "            'col_one':['1.1', '2.2', '3.3'],\n",
    "            'col_two':['4.1', '4.2', '6.3'],\n",
    "            'col_three':['7.1', '8.2', '-']\n",
    "        }\n",
    ")\n",
    "\n",
    "df.dtypes\n",
    "df.astype( {'col_one':'float',  'col_two':'float'}).dtypes\n",
    "pd.to_numeric(df.col_three, errors='coerce')\n",
    "pd.to_numeric(df.col_three, errors='coerce').fillna(0)\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "df\n",
    "\n",
    "#8. Reduce DataFrame size\n",
    "# movies.info(memory_usage='deep')\n",
    "cols = ['date','bedrooms','bathrooms','sqft_living']\n",
    "small_movies = pd.read_excel('data.xls', usecols=cols)\n",
    "# small_movies.info(memory_usage='deep')\n",
    "\n",
    "# cols = ['beer_servings', 'continent']\n",
    "# small_drinks = pd.read_csv('http://bit.ly/drinksbycountry', usecols=cols)\n",
    "# small_drinks.info(memory_usage='deep')\n",
    "\n",
    "\n",
    "#9. Build a DataFrame from multiple files(row-wise), which are all the same format\n",
    "from glob import glob\n",
    "stock_files = sorted(glob('data/stock*.csv'))\n",
    "stock_files\n",
    "pd.concat((pd.read_csv(file) for file in stock_files))\n",
    "stocks = pd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True)\n",
    "\n",
    "\n",
    "#10. Build a DataFrame from multiple files (column-wise)\n",
    "drink_files = sorted(glob('data/drink*.csv'))\n",
    "pd.concat((pd.read_csv(file) for file in drink_files), axis='columns').head()\n",
    "\n",
    "#11. Create a DataFrame from the clipboard which could be copied from spreadsheet.\n",
    "# df = pd.read_clipboard()\n",
    "# df.dtypes\n",
    "# df.index\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "#12. Split a DataFrame into two random subsets. *Note, here Index number must be unique.\n",
    "len( movies)\n",
    "movies_1 = movies.sample(frac=0.75, random_state=1234)\n",
    "movies_2 = movies.drop(movies_1.index)\n",
    "len(movies_1) + len(movies_2)\n",
    "movies_1.index.sort_values()\n",
    "movies_2.index.sort_values()\n",
    "\n",
    "#13. Filter a DataFrame by multiple categories.\n",
    "movies.head()\n",
    "movies.Genre.unique()\n",
    "\n",
    "movies[\n",
    "    (movies.Genre == 'Adventure') |\n",
    "    (movies.Genre == 'Sci-Fi') |\n",
    "    (movies.Genre == 'History') ].head()\n",
    "\n",
    "movies[movies.Genre.isin([ 'Action,Adventure,Sci-Fi', 'Sci-Fi', 'History']) ].head()\n",
    "movies[~movies.Genre.isin(['Action,Adventure,Sci-Fi', 'Sci-Fi', 'History']) ].head()\n",
    "\n",
    "#14. Filter a DataFrame by largest categories\n",
    "counts = movies.Genre.value_counts()\n",
    "counts\n",
    "counts.nlargest(3)\n",
    "counts.nlargest(3).index\n",
    "movies[movies.Genre.isin(counts.nlargest(3).index)].head()\n",
    "\n",
    "\n",
    "#15. Handle missing values\n",
    "mo = pd.read_csv('Movie.csv')\n",
    "mo.head()\n",
    "mo.isna().sum()\n",
    "mo.isna().mean()                # show in persentage\n",
    "mo.dropna(axis='columns').head()\n",
    "\n",
    "    # drop columns if more than 10 % are missing\n",
    "mo.dropna(thresh=len(mo)*0.9, axis='columns').head()\n",
    "mo.dropna(thresh=len(mo)*0.9, axis='columns').isna().sum()\n",
    "\n",
    "\n",
    "#16 Split a string into multiple columns\n",
    "df = pd.DataFrame( \n",
    "    {\n",
    "    'name':['John Arthur Doe', 'Jane Ann Smith'],\n",
    "    'location':['Los Angeles, CA', 'Washington, DC']\n",
    "    }\n",
    ")\n",
    "df.name.str.split(' ', expand=True)\n",
    "df[['first','middle','last']] = df.name.str.split(' ', expand=True)\n",
    "df\n",
    "\n",
    "df.location.str.split(', ', expand=True)\n",
    "df['city'] = df.location.str.split(', ', expand=True)[0]\n",
    "df\n",
    "\n",
    "#17. Expand a Series of lists into a DataFrame \n",
    "df = pd.DataFrame(\n",
    "        {\n",
    "            'col_one':['a','b','c'],\n",
    "            'col_two':[[10,40], [20,50], [30, 60]]\n",
    "        }\n",
    ")\n",
    "df\n",
    "df_new = df.col_two.apply(pd.Series)\n",
    "df_new\n",
    "pd.concat([df, df_new], axis='columns')\n",
    "\n",
    "#18. Aggregate by multiple functions\n",
    "orders = pd.read_csv('data/orders.csv')\n",
    "orders.head()\n",
    "orders[orders.order_id == 1].item_price.sum()\n",
    "orders.groupby('order_id').item_price.sum().head()\n",
    "orders.groupby('order_id').item_price.agg(['sum', 'count']).head()\n",
    "\n",
    "\n",
    "#19. Combine the output of an aggregation with a DataFrame \n",
    "orders.groupby('order_id').item_price.sum().head\n",
    "len(orders.groupby('order_id').item_price.sum())\n",
    "len(orders.item_price)\n",
    "total_price = orders.groupby('order_id').item_price.transform('sum')*orders.quantity\n",
    "total_price\n",
    "len(total_price)\n",
    "orders['total_price'] = total_price\n",
    "orders['percent_of_total'] = orders.item_price / orders.total_price\n",
    "orders.head()\n",
    "\n",
    "#20. Select of slice of rows and columns\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "titanic.head()\n",
    "titanic.describe()    # describe gives more information about numeric data\n",
    "titanic.describe().loc['min':'max']\n",
    "titanic.describe().loc['min':'max', 'PassengerId':'Fare':2]\n",
    "\n",
    "# #21. Reshape a MultiIndexed Series\n",
    "titanic.Survived.mean()\n",
    "titanic.groupby(['Sex', 'Pclass']).Survived.mean()\n",
    "titanic.groupby(['Sex', 'Pclass']).Survived.mean().unstack()\n",
    "\n",
    "#22. Create a pivot table\n",
    "titanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='mean')\n",
    "titanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='mean', margins=True)\n",
    "titanic.pivot_table(index='Sex', columns='Pclass', values='Survived', aggfunc='count', margins=True)\n",
    "\n",
    "#23. Convert continous data into categorical data\n",
    "titanic.Age.head(10)\n",
    "pd.cut(titanic.Age, bins=[0, 18, 25, 99], labels=['child', 'young adult', 'adult']).head(10)\n",
    "\n",
    "#24. Change display option\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "titanic.head()\n",
    "\n",
    "\n",
    "#25. Style a DataFrame \n",
    "# stocks\n",
    "format_dict = {'Date':'{:%m/%d/%y}'}\n",
    "# stocks.style.format(format_dict)\n",
    "stocks.style.hide_index().highlight_min('Close', color='red').highlight_max('Close', color='lightgreen')\n",
    "stocks.style.hide_index().background_gradient(subset='Volume', cmap='Blues')\n",
    "stocks.style.hide_index().bar('Volume', color='lightblue', align='zero').set_caption('stock Prices from October 2016')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
